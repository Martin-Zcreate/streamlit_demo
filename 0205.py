import cv2
import numpy as np
import av
import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from cvzone.HandTrackingModule import HandDetector

# --- é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="äº‘ç«¯AIç¥ç¬”", layout="wide")
st.title("ğŸ–ï¸ äº‘ç«¯ç‰ˆ AI ç¥ç¬”é©¬è‰¯")
st.info("æç¤ºï¼šè¯·å…è®¸æµè§ˆå™¨è®¿é—®æ‘„åƒå¤´ã€‚é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦ 10-20 ç§’ã€‚")

# --- å®šä¹‰ç”»ç¬”å‚æ•° ---
# æ³¨æ„ï¼šåœ¨WebRTCè¿è¡Œæ—¶ï¼Œå®æ—¶ä¿®æ”¹ä¾§è¾¹æ å‚æ•°æ¯”è¾ƒå¤æ‚ï¼Œ
# ä¸ºäº†æ¼”ç¤ºç¨³å®šï¼Œæˆ‘ä»¬æŠŠå‚æ•°å›ºå®šæˆ–ç®€åŒ–
draw_color = (0, 255, 0) # ç»¿è‰² (B, G, R)
brush_thickness = 15

# --- æ ¸å¿ƒå¤„ç†ç±» ---
# è¿™é‡Œä¸å†æ˜¯ç”¨ while å¾ªç¯ï¼Œè€Œæ˜¯å®šä¹‰ä¸€ä¸ªâ€œå¤„ç†å™¨â€
class HandTrackProcessor(VideoTransformerBase):
    def __init__(self):
        # åˆå§‹åŒ–æ‰‹éƒ¨æ£€æµ‹å™¨
        self.detector = HandDetector(detectionCon=0.8, maxHands=1)
        # åˆå§‹åŒ–ç”»å¸ƒ (Canvas)
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç¡®å®šæ‘„åƒå¤´åˆ†è¾¨ç‡ï¼Œå…ˆè®¾ä¸ºNoneï¼Œç¬¬ä¸€å¸§æ¥äº†å†åˆ›å»º
        self.canvas = None
        # ä¸Šä¸€å¸§çš„åæ ‡ç‚¹
        self.xp, self.yp = 0, 0

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        """
        è¿™ä¸ªå‡½æ•°ä¼šé’ˆå¯¹æ¯ä¸€å¸§è§†é¢‘è¢«è°ƒç”¨ä¸€æ¬¡
        """
        # 1. æŠŠæ¥è‡ªç½‘ç»œçš„å¸§è½¬æ¢ä¸º OpenCV å›¾åƒæ ¼å¼
        img = frame.to_ndarray(format="bgr24")
        
        # ç¿»è½¬é•œåƒ
        img = cv2.flip(img, 1)
        
        # 2. åˆå§‹åŒ–ç”»å¸ƒï¼ˆå¦‚æœè¿˜æ²¡åˆ›å»ºï¼‰
        if self.canvas is None:
            # åˆ›å»ºä¸€ä¸ªå’Œå½“å‰è§†é¢‘å¸§ä¸€æ ·å¤§å°çš„é»‘åº•ç”»å¸ƒ
            self.canvas = np.zeros_like(img)

        # 3. AIæ‰‹éƒ¨è¯†åˆ«
        hands, img = self.detector.findHands(img, flipType=False, draw=True)

        if hands:
            lmList = hands[0]['lmList']
            # é£ŸæŒ‡æŒ‡å°–(8) å’Œ ä¸­æŒ‡æŒ‡å°–(12)
            x1, y1 = lmList[8][0], lmList[8][1]
            x2, y2 = lmList[12][0], lmList[12][1]

            # åˆ¤æ–­æ‰‹æŒ‡çŠ¶æ€
            fingers = self.detector.fingersUp(hands[0])

            # --- é€»è¾‘å¤ç”¨ä¹‹å‰çš„ ---
            # æ¨¡å¼Aï¼šæš‚åœ (é£ŸæŒ‡+ä¸­æŒ‡)
            if fingers[1] and fingers[2]:
                self.xp, self.yp = 0, 0
                cv2.circle(img, (x1, y1), 25, draw_color, cv2.FILLED)

            # æ¨¡å¼Bï¼šç»˜ç”» (ä»…é£ŸæŒ‡)
            elif fingers[1] and not fingers[2]:
                if self.xp == 0 and self.yp == 0:
                    self.xp, self.yp = x1, y1
                
                # åœ¨ self.canvas ä¸Šç”»çº¿
                cv2.line(self.canvas, (self.xp, self.yp), (x1, y1), draw_color, brush_thickness)
                self.xp, self.yp = x1, y1

        # 4. å›¾åƒèåˆ (æŠŠç”»å¸ƒå åŠ åˆ°æ‘„åƒå¤´ç”»é¢)
        imgGray = cv2.cvtColor(self.canvas, cv2.COLOR_BGR2GRAY)
        _, imgInv = cv2.threshold(imgGray, 50, 255, cv2.THRESH_BINARY_INV)
        imgInv = cv2.cvtColor(imgInv, cv2.COLOR_GRAY2BGR)

        img = cv2.bitwise_and(img, imgInv)
        img = cv2.bitwise_or(img, self.canvas)

        # 5. æŠŠå¤„ç†å¥½çš„ OpenCV å›¾åƒè½¬å› WebRTC å¸§è¿”å›ç»™æµè§ˆå™¨
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- å¯åŠ¨ WebRTC ç»„ä»¶ ---
# rtc_configuration ç”¨äºé…ç½®ç©¿é€æœåŠ¡å™¨(STUN/TURN)ï¼Œ
# åœ¨æŸäº›å…¬å¸å†…ç½‘æˆ–æ ¡å›­ç½‘å¯èƒ½å› ä¸ºé˜²ç«å¢™æ— æ³•è¿æ¥ï¼Œ
# è¿™é‡Œä½¿ç”¨ Google å…è´¹çš„ STUN æœåŠ¡å™¨å°è¯•è¿æ¥ã€‚
rtc_configuration = {
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
}

webrtc_streamer(
    key="hand-drawing",
    video_processor_factory=HandTrackProcessor,
    rtc_configuration=rtc_configuration,
    media_stream_constraints={"video": True, "audio": False}, # åªè¦è§†é¢‘ï¼Œä¸è¦éŸ³é¢‘
)

st.markdown("---")
st.write("æ“ä½œè¯´æ˜ï¼š")
st.write("1. ç‚¹å‡» START æŒ‰é’®å¼€å¯æ‘„åƒå¤´ã€‚")
st.write("2. ä¼¸å‡º**é£ŸæŒ‡**è¿›è¡Œç»˜ç”»ã€‚")
st.write("3. åŒæ—¶ä¼¸å‡º**é£ŸæŒ‡å’Œä¸­æŒ‡**æš‚åœç»˜ç”»ã€‚")
st.write("4. ç‚¹å‡» STOP å†ç‚¹å‡» START å¯ä»¥æ¸…ç©ºç”»å¸ƒã€‚")
