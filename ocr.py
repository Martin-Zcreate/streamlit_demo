import streamlit as st
import base64
from openai import OpenAI
from PIL import Image

def get_img_str(file_path):
    with open(file_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

# ä½¿ç”¨ cache_data é¿å…é‡å¤è¯†åˆ«ï¼ŒèŠ‚çœ token ä¸”ä¼˜åŒ–ä½“éªŒ
@st.cache_data(show_spinner=False)
def get_ocr_text(file_content):
    # æ³¨æ„ï¼šst.cache_data å¯¹ bytes æ›´å‹å¥½ï¼Œæ‰€ä»¥ä¼ å…¥ content è€Œä¸æ˜¯ UploadedFile å¯¹è±¡
    a = "sk-vogujjwsiclsbtlaorwvnncwfidlxavtukoxcqlciakmhtkr"
    b = "deepseek-ai/DeepSeek-OCR"
    
    # å»ºç«‹è¿æ¥
    client = OpenAI(
        api_key=a,
        base_url="https://api.siliconflow.cn/v1"
    )

    try:
        # d: å›¾ç‰‡çš„ Base64 ç¼–ç 
        d = base64.b64encode(file_content).decode('utf-8')
        
        print(f"æ­£åœ¨å‘é€è¯·æ±‚ç»™æ¨¡å‹: {b} ...")

        # e: å‘é€è¯·æ±‚
        e = client.chat.completions.create(
            model=b,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¯·å°†è¿™å¼ å›¾ç‰‡é‡Œçš„æ‰€æœ‰æ–‡å­—å’Œæ•°å­¦å…¬å¼æå–å‡ºæ¥ã€‚é‡è¦ï¼šå…¬å¼è¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $ åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ $$ åŒ…è£¹ï¼Œä¸è¦è¾“å‡ºå¤šä½™çš„Markdownæ ‡è®°ã€‚"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{d}"}}
                    ]
                }
            ],
            temperature=0.1,
        )

        return e.choices[0].message.content

    except Exception as err:
        st.error(f"OCRå‡ºé”™: {err}")
        return None

def AI(messages):
    client = OpenAI(api_key="sk-af6ba48dbd8a4d1fb0d036551b9bbdc3",
                    base_url="https://api.deepseek.com")
    
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        stream=True
    )
    return response

# ================= ç½‘é¡µç•Œé¢å¸ƒå±€ =================

st.set_page_config(page_title="æ™ºé…·AIä½œä¸šå¸®æ‰‹", page_icon="ğŸ¤–")
st.title("ğŸ¤–æ™ºé…·AIä½œä¸šå¸®æ‰‹")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_topic" not in st.session_state:
    st.session_state.current_topic = None
if "last_uploaded_file_id" not in st.session_state:
    st.session_state.last_uploaded_file_id = None

st.markdown("""
<style>
/* ä¼˜åŒ–ä¸Šä¼ æŒ‰é’®æ ·å¼ */
div[data-testid="stFileUploader"] label {
    font-size: 1.1rem !important;
    font-weight: bold !important;
}
</style>
""", unsafe_allow_html=True)

st.info("ğŸ’¡ æç¤ºï¼šç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ï¼Œç›´æ¥é€‰æ‹©ã€æ‹ç…§ã€‘æˆ–ã€ç›¸æœºã€‘ä»¥ä¸Šä¼ é¢˜ç›®ã€‚")

# 1. åªä¿ç•™ä¸Šä¼ /ç³»ç»Ÿç›¸æœºæ¨¡å¼
img_file = st.file_uploader(
    "  ç‚¹å‡»æ‹æ‘„é¢˜ç›®", 
    type=['jpg', 'png', 'jpeg'], 
    accept_multiple_files=False,
    key="uploader"
)

# å¤„ç†å›¾ç‰‡ä¸Šä¼ é€»è¾‘
if img_file:
    # ç®€å•çš„æ–‡ä»¶IDç”Ÿæˆï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯æ–°æ–‡ä»¶
    file_content = img_file.getvalue()
    file_id = f"{img_file.name}_{img_file.size}"
    
    # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¿›è¡Œ OCR å’Œåˆå§‹åŒ–
    if file_id != st.session_state.last_uploaded_file_id:
        with st.spinner('æ­£åœ¨è¯†åˆ«é¢˜ç›®...'):
            ocr_result = get_ocr_text(file_content)
            
            if ocr_result:
                st.session_state.current_topic = ocr_result
                st.session_state.last_uploaded_file_id = file_id
                
                # åˆå§‹åŒ–æ–°çš„å¯¹è¯
                st.session_state.messages = [
                    {"role": "system", "content": """
                    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è€å¸ˆã€‚
                    1. å½“å­¦ç”Ÿé—®é—®é¢˜æ—¶ï¼Œä¸è¦ç›´æ¥ç»™å®Œæ•´ç­”æ¡ˆã€‚
                    2. è¯·ä½¿ç”¨"éš¾åº¦é™çº§"æ³•ï¼ŒæŠŠå¤æ‚çš„é¢˜ç›®æ‹†è§£æˆç®€å•çš„æ­¥éª¤ã€‚
                    3. å…ˆè§£é‡Šæ€è·¯ï¼Œå†è®©å­¦ç”Ÿå»æ€è€ƒé—®é¢˜ã€‚
                    4. å…¬å¼è¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $ åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ $$ åŒ…è£¹ã€‚
                    """},
                    {"role": "user", "content": f"å­¦ç”Ÿå‘æ¥äº†è¿™é“é¢˜ï¼Œè¯·è®²è§£ï¼š\n{ocr_result}"}
                ]
                
                # è‡ªåŠ¨è§¦å‘ç¬¬ä¸€æ¬¡è®²è§£
                with st.spinner('è€å¸ˆæ­£åœ¨æ€è€ƒ...'):
                    # å ä½ç¬¦ç”¨äºæµå¼è¾“å‡º
                    full_response = ""
                    # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥æ˜¾ç¤ºï¼Œè€Œæ˜¯é€šè¿‡ rerun è®©ä¸‹é¢çš„èŠå¤©å¾ªç¯å¤„ç†
                    # ä½†ä¸ºäº†ç”¨æˆ·ä½“éªŒï¼Œé¦–æ¬¡å¯ä»¥ç›´æ¥è°ƒç”¨å¹¶å­˜å…¥ history
                    stream = AI(st.session_state.messages)
                    for chunk in stream:
                        if chunk.choices[0].delta.content:
                            full_response += chunk.choices[0].delta.content
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

# 2. æ˜¾ç¤ºè¯†åˆ«åˆ°çš„é¢˜ç›®ï¼ˆä¼˜åŒ–æ˜¾ç¤ºï¼‰
if st.session_state.current_topic:
    with st.expander("ğŸ“ æŸ¥çœ‹è¯†åˆ«åˆ°çš„é¢˜ç›®", expanded=True):
        # ä½¿ç”¨ markdown æ¸²æŸ“ LaTeX
        st.markdown(st.session_state.current_topic)

# 3. èŠå¤©ç•Œé¢
st.subheader("ğŸ‘¨â€ğŸ« è€å¸ˆè®²è§£ & ç­”ç–‘")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯ (è·³è¿‡ system æ¶ˆæ¯å’Œç¬¬ä¸€æ¡åŒ…å«å¤§é‡ prompt çš„ user æ¶ˆæ¯ï¼Œåªæ˜¾ç¤ºæ ¸å¿ƒå†…å®¹)
for msg in st.session_state.messages:
    if msg["role"] == "system":
        continue
    # å¯¹äºç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼ˆåŒ…å«"å­¦ç”Ÿå‘æ¥äº†è¿™é“é¢˜..."ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³é‡å¤æ˜¾ç¤ºï¼Œæˆ–è€…ç®€åŒ–æ˜¾ç¤º
    # è¿™é‡Œç®€å•èµ·è§ï¼Œå…¨éƒ¨æ˜¾ç¤ºï¼Œæˆ–è€…ä½ å¯ä»¥é€‰æ‹©éšè—ç¬¬ä¸€æ¡ user æ¶ˆæ¯
    if msg["role"] == "user" and "å­¦ç”Ÿå‘æ¥äº†è¿™é“é¢˜" in msg["content"]:
        continue 
        
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# åº•éƒ¨è¾“å…¥æ¡†
if prompt := st.chat_input("å“ªé‡Œä¸æ‡‚ï¼Ÿå¯ä»¥ç»§ç»­é—®è€å¸ˆ..."):
    # æ˜¾ç¤ºç”¨æˆ·æé—®
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # æ˜¾ç¤º AI å›å¤
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        stream = AI(st.session_state.messages)
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
