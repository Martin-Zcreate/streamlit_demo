import streamlit as st
import base64
import re
import io
from PIL import Image
from openai import OpenAI

def clean_latex(text):
    """
    å°† LaTeX çš„ \( ... \) è½¬æ¢ä¸º $ ... $
    å°† \[ ... \] è½¬æ¢ä¸º $$ ... $$
    ä»¥ä¾¿ Streamlit æ­£ç¡®æ¸²æŸ“
    """
    if not text:
        return text
    # æ›¿æ¢å—çº§å…¬å¼ \[ ... \] ä¸º $$ ... $$
    text = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    # æ›¿æ¢è¡Œå†…å…¬å¼ \( ... \) ä¸º $ ... $
    text = re.sub(r'\\\((.*?)\\\)', r'$\1$', text, flags=re.DOTALL)
    return text

def compress_image(image_bytes, max_size_kb=150):
    """
    å¦‚æœå›¾ç‰‡è¶…è¿‡ max_size_kbï¼Œåˆ™è¿›è¡Œå‹ç¼©
    """
    try:
        current_size = len(image_bytes)
        if current_size <= max_size_kb * 1024:
            return image_bytes

        st.toast(f"å›¾ç‰‡å¤§å° {current_size/1024:.1f}KB > {max_size_kb}KBï¼Œæ­£åœ¨å‹ç¼©...", icon="ğŸ“‰")
        
        img = Image.open(io.BytesIO(image_bytes))
        
        # è½¬æ¢ä¸º RGB (å…¼å®¹ PNG/RGBA)
        if img.mode in ("RGBA", "P"):
            img = img.convert("RGB")
            
        # å¾ªç¯å‹ç¼©ç›´åˆ°æ»¡è¶³å¤§å°
        quality = 90
        width, height = img.size
        scale = 1.0
        
        while True:
            output_buffer = io.BytesIO()
            # è°ƒæ•´å°ºå¯¸
            if scale < 1.0:
                new_width = int(width * scale)
                new_height = int(height * scale)
                resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                resized_img.save(output_buffer, format="JPEG", quality=quality)
            else:
                img.save(output_buffer, format="JPEG", quality=quality)
            
            compressed_bytes = output_buffer.getvalue()
            
            if len(compressed_bytes) <= max_size_kb * 1024:
                return compressed_bytes
            
            # å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œé™ä½è´¨é‡æˆ–å°ºå¯¸
            if quality > 60:
                quality -= 10
            else:
                # è´¨é‡å·²ç»å¾ˆä½äº†ï¼Œå¼€å§‹ç¼©å°ºå¯¸
                scale *= 0.8
                
            # é¿å…æ­»å¾ªç¯
            if scale < 0.1:
                return compressed_bytes

    except Exception as e:
        st.warning(f"å›¾ç‰‡å‹ç¼©å¼‚å¸¸: {e}")
        return image_bytes

def get_img_str(file_path):
    with open(file_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def get_ocr_text(uploaded_file):
    a = "sk-vogujjwsiclsbtlaorwvnncwfidlxavtukoxcqlciakmhtkr"
    b = "deepseek-ai/DeepSeek-OCR" 

    # å»ºç«‹è¿æ¥ (ç¡…åŸºæµåŠ¨ä¸“ç”¨åœ°å€)
    client = OpenAI(
        api_key=a, 
        base_url="https://api.siliconflow.cn/v1"
    )

    # è¾…åŠ©å‡½æ•°ï¼šæŠŠå›¾ç‰‡è½¬æˆå­—ç¬¦ä¸²
    

    try:
        # d: å›¾ç‰‡çš„ Base64 ç¼–ç 
        raw_bytes = uploaded_file.getvalue()
        
        # å‹ç¼©å¤„ç† (å¦‚æœ > 150KB)
        processed_bytes = compress_image(raw_bytes, max_size_kb=150)
        
        d = base64.b64encode(processed_bytes).decode('utf-8')
        
        print(f"æ­£åœ¨å‘é€è¯·æ±‚ç»™æ¨¡å‹: {b} ...")

        # e: å‘é€è¯·æ±‚
        e = client.chat.completions.create(
            model=b,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¯·å°†è¿™å¼ å›¾ç‰‡é‡Œçš„æ‰€æœ‰æ–‡å­—å’Œæ•°å­¦å…¬å¼æå–å‡ºæ¥ã€‚å…¬å¼è¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $ åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ $$ åŒ…è£¹ã€‚"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{d}"}}
                    ]
                }
            ],
            temperature=0.1, # 0.1 è®©å®ƒä¸¥è°¨ç‚¹ï¼Œåˆ«ä¹±å‘æŒ¥
        )

        # æ‰“å°ç»“æœ

        content = e.choices[0].message.content
        return clean_latex(content)

    except Exception as err:
        st.error(f"OCRå‡ºé”™: {err}")
        return None


def AI_stream(messages):
    client = OpenAI(api_key="sk-af6ba48dbd8a4d1fb0d036551b9bbdc3",
                    base_url="https://api.deepseek.com")
    
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        stream=True
    )
    return response

# ================= ç½‘é¡µç•Œé¢å¸ƒå±€ =================

st.title("ğŸ¤– AI ä½œä¸šå¸®æ‰‹")
st.write("ç”¨æ‰‹æœºæ‹ä¸‹é¢˜ç›®ï¼ŒAI å¸®ä½ æ‹†è§£æ€è·¯ã€‚")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_uploaded_file" not in st.session_state:
    st.session_state.last_uploaded_file = None
if "ocr_result" not in st.session_state:
    st.session_state.ocr_result = None

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è€å¸ˆã€‚
1. å½“å­¦ç”Ÿé—®é—®é¢˜æ—¶ï¼Œä¸è¦ç›´æ¥ç»™å®Œæ•´ä»£ç æˆ–ç­”æ¡ˆã€‚
2. è¯·ä½¿ç”¨"éš¾åº¦é™çº§"æ³•ï¼ŒæŠŠå¤æ‚çš„é¢˜ç›®æ‹†è§£æˆç®€å•çš„æ­¥éª¤ã€‚
3. å¦‚æœæ˜¯ç¼–ç¨‹é¢˜ï¼Œå…ˆè§£é‡Šæ€è·¯ï¼Œå†è®©å­¦ç”Ÿå»æ€è€ƒä»£ç ã€‚
4. å…¬å¼è¯·ä½¿ç”¨ LaTeX æ ¼å¼ï¼Œè¡Œå†…å…¬å¼ç”¨ $ åŒ…è£¹ï¼Œç‹¬ç«‹å…¬å¼ç”¨ $$ åŒ…è£¹ã€‚
"""

# 1. æ‰‹æœºè°ƒç”¨æ‘„åƒå¤´ç»„ä»¶
img_file = st.file_uploader(
    "ğŸ“¸ ç‚¹å‡»æ‹æ‘„é¢˜ç›® (æˆ–åœ¨å·¦ä¾§é€‰æ‹©å…¶ä»–åŠŸèƒ½)", 
    type=['jpg', 'png', 'jpeg'], 
    accept_multiple_files=False,
    key="uploader"
)

if img_file:
    # ç”Ÿæˆæ–‡ä»¶æŒ‡çº¹ (ç®€å•ç”¨ name + size)
    file_id = f"{img_file.name}-{img_file.size}"
    
    # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œé‡ç½®çŠ¶æ€
    if st.session_state.last_uploaded_file != file_id:
        st.session_state.last_uploaded_file = file_id
        st.session_state.messages = []
        st.session_state.ocr_result = None
        
        # æ‰§è¡Œ OCR
        with st.spinner('æ­£åœ¨è¯†åˆ«é¢˜ç›®...'):
            st.session_state.ocr_result = get_ocr_text(img_file)

    # å¦‚æœæœ‰è¯†åˆ«ç»“æœ
    if st.session_state.ocr_result:
        f = st.session_state.ocr_result
        
        # æ˜¾ç¤ºè¯†åˆ«ç»“æœç»™ç”¨æˆ·ç¡®è®¤
        st.subheader("ğŸ“ è¯†åˆ«åˆ°çš„é¢˜ç›®")
        st.markdown(f)
        
        # ----------------- å¯¹è¯åŒºåŸŸ -----------------
        st.subheader("ğŸ‘¨â€ğŸ« è€å¸ˆè®²è§£ & ç­”ç–‘")
        
        # å¦‚æœå†å²ä¸ºç©ºï¼Œè¯´æ˜æ˜¯åˆšè¯†åˆ«å®Œï¼Œè‡ªåŠ¨è§¦å‘ç¬¬ä¸€æ¬¡è®²è§£
        if not st.session_state.messages:
            initial_user_msg = f"å­¦ç”Ÿå‘æ¥äº†è¿™é“é¢˜ï¼Œè¯·è®²è§£ï¼š\n{f}"
            
            # å­˜å…¥ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆä½†ä¸åœ¨ç•Œé¢ä¸Šé‡å¤æ˜¾ç¤ºï¼Œå› ä¸ºä¸Šé¢å·²ç»æ˜¾ç¤ºäº†é¢˜ç›®ï¼‰
            st.session_state.messages.append({"role": "user", "content": initial_user_msg})
            
            # æ„é€  API è¯·æ±‚æ¶ˆæ¯
            api_messages = [{"role": "system", "content": SYSTEM_PROMPT}] + st.session_state.messages
            
            # æ˜¾ç¤º AI å›å¤å®¹å™¨
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                # è°ƒç”¨ AI
                try:
                    stream = AI_stream(api_messages)
                    for chunk in stream:
                        if chunk.choices[0].delta.content:
                            full_response += chunk.choices[0].delta.content
                            message_placeholder.markdown(clean_latex(full_response))
                    
                    # å­˜å…¥å†å²
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"AI å“åº”å‡ºé”™: {e}")

        else:
            # å¦‚æœå·²æœ‰å†å²ï¼Œæ¸²æŸ“å†å²æ¶ˆæ¯
            for i, msg in enumerate(st.session_state.messages):
                # è·³è¿‡ç¬¬ä¸€æ¡ User æ¶ˆæ¯ï¼ˆå› ä¸ºå®ƒæ˜¯é¢˜ç›®ï¼Œå·²ç»æ˜¾ç¤ºåœ¨ä¸Šé¢äº†ï¼‰
                if i == 0 and msg["role"] == "user":
                    continue
                
                with st.chat_message(msg["role"]):
                    st.markdown(clean_latex(msg["content"]))

        # ----------------- åº•éƒ¨è¾“å…¥æ¡† -----------------
        if prompt := st.chat_input("è¿˜æœ‰å“ªé‡Œä¸æ‡‚ï¼Ÿç»§ç»­é—®è€å¸ˆ..."):
            # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
            with st.chat_message("user"):
                st.markdown(prompt)
            # å­˜å…¥å†å²
            st.session_state.messages.append({"role": "user", "content": prompt})

            # 2. ç”Ÿæˆ AI å›å¤
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                # æ„é€  API è¯·æ±‚
                api_messages = [{"role": "system", "content": SYSTEM_PROMPT}] + st.session_state.messages
                
                try:
                    stream = AI_stream(api_messages)
                    for chunk in stream:
                        if chunk.choices[0].delta.content:
                            full_response += chunk.choices[0].delta.content
                            message_placeholder.markdown(clean_latex(full_response))
                    
                    # å­˜å…¥å†å²
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                except Exception as e:
                    st.error(f"AI å“åº”å‡ºé”™: {e}")
