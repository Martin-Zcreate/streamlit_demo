import streamlit as st
import base64
import re
import io
from PIL import Image, ImageOps
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================
# âš ï¸ è¯·å¡«å…¥ä½ çš„ç¡…åŸºæµåŠ¨ API Key
# æ³¨å†Œåœ°å€: https://cloud.siliconflow.cn/
API_KEY = "sk-vogujjwsiclsbtlaorwvnncwfidlxavtukoxcqlciakmhtkr" 

# æ¨¡å‹é…ç½®
OCR_MODEL = "deepseek-ai/DeepSeek-OCR"
CHAT_MODEL = "deepseek-ai/DeepSeek-V3" # æˆ–è€… deepseek-ai/DeepSeek-R1

# ================= å·¥å…·å‡½æ•° =================

def clean_latex(text):
    """
    æ¸…æ´— LaTeX æ ¼å¼ä»¥ä¾¿ Streamlit æ­£ç¡®æ¸²æŸ“
    """
    if not text:
        return ""
    # æ›¿æ¢å—çº§å…¬å¼ \[ ... \] -> $$ ... $$
    text = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    # æ›¿æ¢è¡Œå†…å…¬å¼ \( ... \) -> $ ... $
    text = re.sub(r'\\\((.*?)\\\)', r'$\1$', text, flags=re.DOTALL)
    # ç§»é™¤ Markdown ä»£ç å—æ ‡è®°ï¼Œé˜²æ­¢å…¬å¼è¢«åŒ…è£¹åœ¨ä»£ç å—é‡Œä¸æ¸²æŸ“
    text = re.sub(r'```latex', '', text)
    text = re.sub(r'```', '', text)
    return text

def process_image(image_bytes, max_mb=4):
    """
    å›¾ç‰‡é¢„å¤„ç†ç»ˆæç‰ˆï¼š
    1. ä¿®æ­£ EXIF æ—‹è½¬ (æ‰‹æœºæ‹ç…§å¿…åš)
    2. ä¿®å¤ PNG é€æ˜èƒŒæ™¯å˜é»‘ (è½¬ç™½åº•)
    3. æ™ºèƒ½å‹ç¼©ï¼šä»…å½“å›¾ç‰‡ > 4MB æ—¶æ‰å‹ç¼©ï¼Œæœ€å¤§ç¨‹åº¦ä¿ç•™ç»†èŠ‚
    """
    try:
        img = Image.open(io.BytesIO(image_bytes))
        
        # 1. ä¿®æ­£æ—‹è½¬ (æ‰‹æœºç«–æ‹ç…§ç‰‡å¸¸å¸¦æ—‹è½¬è§’)
        img = ImageOps.exif_transpose(img)
        
        # 2. å¤„ç†é¢œè‰²æ¨¡å¼ (RGBAè½¬RGBï¼Œé€æ˜å˜ç™½)
        if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):
            bg = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            bg.paste(img, mask=img.split()[-1]) # ä½¿ç”¨ alpha é€šé“åšæ©ç 
            img = bg
        elif img.mode != 'RGB':
            img = img.convert('RGB')

        # 3. æ£€æŸ¥å¤§å°
        buf = io.BytesIO()
        img.save(buf, format="JPEG", quality=95) # é»˜è®¤é«˜è´¨é‡
        size_bytes = len(buf.getvalue())
        limit_bytes = max_mb * 1024 * 1024
        
        # å¦‚æœå°äºé™åˆ¶ï¼Œç›´æ¥è¿”å›
        if size_bytes <= limit_bytes:
            return buf.getvalue()

        # 4. è¶…å‡ºé™åˆ¶åˆ™å¾ªç¯å‹ç¼©
        quality = 90
        scale = 0.9
        w, h = img.size
        
        while size_bytes > limit_bytes:
            buf = io.BytesIO()
            nw, nh = int(w * scale), int(h * scale)
            resized = img.resize((nw, nh), Image.Resampling.LANCZOS)
            resized.save(buf, format="JPEG", quality=quality)
            
            size_bytes = len(buf.getvalue())
            
            scale *= 0.8
            if scale < 0.3: break 
            
        return buf.getvalue()

    except Exception as e:
        st.error(f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}")
        return image_bytes

def get_ocr_text(image_bytes):
    """
    è°ƒç”¨ OCRï¼Œå‚æ•°ä¸¥æ ¼å¯¹é½ Playground æˆªå›¾
    """
    client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")

    try:
        b64_img = base64.b64encode(image_bytes).decode('utf-8')
        
        response = client.chat.completions.create(
            model=OCR_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¯·æå–å›¾ä¸­æ‰€æœ‰å†…å®¹ï¼Œæ•°å­¦å…¬å¼è¯·åŠ¡å¿…ä½¿ç”¨ LaTeX æ ¼å¼ï¼ˆè¡Œå†…ç”¨ $ï¼Œç‹¬å è¡Œç”¨ $$ï¼‰ã€‚ä¸è¦åŒ…å«åŸæœ¬æ²¡æœ‰çš„è§£é‡Šæ–‡å­—ã€‚"},
                        {
                            "type": "image_url", 
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{b64_img}",
                                "detail": "high" # âš¡ï¸å…³é”®ï¼šå¼ºåˆ¶é«˜æ¸…æ¨¡å¼
                            }
                        }
                    ]
                }
            ],
            # === æ ¹æ®æˆªå›¾è°ƒæ•´çš„å‚æ•° ===
            temperature=0.0,       # æˆªå›¾è®¾ç½®ï¼š0.0
            top_p=0.7,             # æˆªå›¾è®¾ç½®ï¼š0.7
            max_tokens=4096,       # æˆªå›¾è®¾ç½®ï¼š4096
            frequency_penalty=0.0  # æˆªå›¾è®¾ç½®ï¼š0.0
            # ========================
        )
        
        return clean_latex(response.choices[0].message.content)

    except Exception as e:
        st.error(f"OCR è¯·æ±‚å¤±è´¥: {e}")
        return None

def ai_stream(messages):
    """
    è°ƒç”¨å¯¹è¯æ¨¡å‹ (DeepSeek-V3/R1)
    """
    client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")
    
    # æ¸…ç†æ¶ˆæ¯å†å²ï¼Œç¡®ä¿åªå‘é€æ–‡æœ¬ç»™å¯¹è¯æ¨¡å‹ï¼ˆé¿å…æ ¼å¼é”™è¯¯ï¼‰
    text_msgs = []
    for m in messages:
        # åªå–æ–‡æœ¬å†…å®¹
        text_msgs.append({"role": m["role"], "content": m["content"]})

    response = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=text_msgs,
        stream=True,
        temperature=0.7 # è®²é¢˜å¯ä»¥ç¨å¾®çµæ´»ä¸€ç‚¹
    )
    return response

# ================= é¡µé¢ä¸»é€»è¾‘ =================

st.set_page_config(page_title="AI é¢˜ç›®è®²è§£", page_icon="ğŸ“")
st.title("ğŸ“ AI é¢˜ç›®è®²è§£åŠ©æ‰‹")
st.caption("åŸºäº DeepSeek-OCR & DeepSeek-V3 | ç¡…åŸºæµåŠ¨å¼ºåŠ›é©±åŠ¨")

# çŠ¶æ€åˆå§‹åŒ–
if "history" not in st.session_state:
    st.session_state.history = []
if "last_file" not in st.session_state:
    st.session_state.last_file = None
if "ocr_content" not in st.session_state:
    st.session_state.ocr_content = None

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½è€å¿ƒã€ä¸“ä¸šçš„è€å¸ˆã€‚
1. æ‹¿åˆ°é¢˜ç›®å†…å®¹åï¼Œå…ˆæ¢³ç†æ€è·¯ï¼Œå†é€æ­¥è®²è§£ã€‚
2. é‡åˆ°æ•°å­¦å…¬å¼ï¼Œå¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼ï¼šè¡Œå†…ç”¨ $...$ï¼Œç‹¬ç«‹å…¬å¼ç”¨ $$...$$ã€‚
3. è®²è§£è¦æ¸…æ™°æ˜“æ‡‚ï¼Œé€‚åˆå­¦ç”Ÿé˜…è¯»ã€‚
"""

# ä¸Šä¼ ç»„ä»¶
uploaded_file = st.file_uploader("ğŸ“¸ æ‹ç…§æˆ–ä¸Šä¼ å›¾ç‰‡", type=['jpg', 'png', 'jpeg'])

if uploaded_file:
    file_id = f"{uploaded_file.name}-{uploaded_file.size}"
    
    # å‘ç°æ–°å›¾ç‰‡ï¼Œå¼€å§‹å¤„ç†
    if st.session_state.last_file != file_id:
        st.session_state.last_file = file_id
        st.session_state.history = [] # æ¸…ç©ºæ—§èŠå¤©
        st.session_state.ocr_content = None
        
        raw_bytes = uploaded_file.getvalue()
        
        with st.status("ğŸ” æ­£åœ¨åˆ†æå›¾ç‰‡...", expanded=True) as status:
            st.write("ğŸ› ï¸ å›¾ç‰‡é¢„å¤„ç† (æ—‹è½¬ä¿®æ­£/å»å™ª/å°ºå¯¸ä¼˜åŒ–)...")
            processed_bytes = process_image(raw_bytes)
            
            st.write("ğŸš€ æ­£åœ¨è¯†åˆ«æ–‡å­—ä¸å…¬å¼ (DeepSeek-OCR)...")
            ocr_text = get_ocr_text(processed_bytes)
            
            if ocr_text:
                st.session_state.ocr_content = ocr_text
                status.update(label="è¯†åˆ«æˆåŠŸ", state="complete", expanded=False)
                
                # æ„é€ åˆå§‹å¯¹è¯
                init_msg = f"è¿™æ˜¯è¯†åˆ«åˆ°çš„é¢˜ç›®å†…å®¹ï¼š\n\n{ocr_text}\n\nè¯·è€å¸ˆå¸®æˆ‘è®²è§£è¿™é“é¢˜ã€‚"
                # å­˜å…¥ system prompt å’Œ ç¬¬ä¸€æ¡ user msg
                st.session_state.history = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": init_msg}
                ]
                
                # === è‡ªåŠ¨è§¦å‘ç¬¬ä¸€æ¬¡è®²è§£ ===
                with st.chat_message("assistant"):
                    ph = st.empty()
                    full_res = ""
                    try:
                        stream = ai_stream(st.session_state.history)
                        for chunk in stream:
                            txt = chunk.choices[0].delta.content
                            if txt:
                                full_res += txt
                                ph.markdown(clean_latex(full_res) + "â–Œ")
                        ph.markdown(clean_latex(full_res))
                        st.session_state.history.append({"role": "assistant", "content": full_res})
                    except Exception as e:
                        st.error(f"ç”Ÿæˆè®²è§£å‡ºé”™: {e}")
            else:
                status.update(label="è¯†åˆ«å¤±è´¥", state="error")
                st.error("æ— æ³•æå–å†…å®¹ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡æ˜¯å¦æ¸…æ™°ã€‚")

# === ç•Œé¢æ˜¾ç¤º ===

# 1. æ˜¾ç¤ºè¯†åˆ«çš„åŸæ–‡ (å¯æŠ˜å )
if st.session_state.ocr_content:
    with st.expander("æŸ¥çœ‹åŸå§‹ OCR è¯†åˆ«ç»“æœ", expanded=False):
        st.markdown(st.session_state.ocr_content)

# 2. èŠå¤©åŒºåŸŸ
st.divider()

# æ¸²æŸ“å†å²è®°å½• (è·³è¿‡ system å’Œ ç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼Œé¿å…é‡å¤æ˜¾ç¤ºé¢˜ç›®)
for i, msg in enumerate(st.session_state.history):
    if msg["role"] == "system": continue
    # å¦‚æœæƒ³æŠŠç¬¬ä¸€æ¡åŒ…å«é¢˜ç›®å†…å®¹çš„ user æ¶ˆæ¯éšè—ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
    # if i == 1: continue 
    
    with st.chat_message(msg["role"]):
        st.markdown(clean_latex(msg["content"]))

# 3. è¿½é—®è¾“å…¥æ¡†
if prompt := st.chat_input("è¿˜æœ‰å“ªé‡Œä¸æ‡‚ï¼Ÿ"):
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.history.append({"role": "user", "content": prompt})
    
    # AI å›å¤
    with st.chat_message("assistant"):
        ph = st.empty()
        full_res = ""
        try:
            stream = ai_stream(st.session_state.history)
            for chunk in stream:
                txt = chunk.choices[0].delta.content
                if txt:
                    full_res += txt
                    ph.markdown(clean_latex(full_res) + "â–Œ")
            ph.markdown(clean_latex(full_res))
            st.session_state.history.append({"role": "assistant", "content": full_res})
        except Exception as e:
            st.error(f"å›å¤å‡ºé”™: {e}")
