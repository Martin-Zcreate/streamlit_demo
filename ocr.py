import streamlit as st
import base64
import re
import io
import json
from PIL import Image, ImageOps
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================
# âš ï¸ å¡«å…¥ä½ çš„ Key
API_KEY = "sk-vogujjwsiclsbtlaorwvnncwfidlxavtukoxcqlciakmhtkr" 
OCR_MODEL = "deepseek-ai/DeepSeek-OCR"
CHAT_MODEL = "deepseek-ai/DeepSeek-V3"

# ================= æ ¸å¿ƒå·¥å…· =================

def clean_latex(text):
    """
    æ¸…æ´—æ•°æ®ï¼šå¤„ç† LaTeX ç¬¦å·ï¼ŒåŒæ—¶é˜²æ­¢æ¨¡å‹è¿”å› JSON æ ¼å¼å¯¼è‡´æ»¡å±å¤§æ‹¬å·
    """
    if not text:
        return ""

    # 1. ğŸ” é˜²é”™ï¼šå¦‚æœæ¨¡å‹è¿”å›äº† JSON æ ¼å¼ (ä¾‹å¦‚ {"content": "..."})ï¼Œå°è¯•æå–å†…éƒ¨æ–‡æœ¬
    text = text.strip()
    if text.startswith("{") and text.endswith("}"):
        try:
            data = json.loads(text)
            # å°è¯•æ‰¾å¸¸è§çš„å­—æ®µå
            if "content" in data: text = data["content"]
            elif "text" in data: text = data["text"]
        except:
            pass # è§£æå¤±è´¥å°±ç®—äº†ï¼ŒæŒ‰åŸæ ·å¤„ç†

    # 2. ğŸ§¹ ç§»é™¤ Markdown ä»£ç å—åŒ…è£¹ (```json æˆ– ```latex)
    text = re.sub(r'^```\w*\n', '', text) # å»å¤´
    text = re.sub(r'\n```$', '', text)    # å»å°¾

    # 3. ğŸ“ ä¿®æ­£å…¬å¼æ ¼å¼
    text = re.sub(r'\\\[(.*?)\\\]', r'$$\1$$', text, flags=re.DOTALL)
    text = re.sub(r'\\\((.*?)\\\)', r'$\1$', text, flags=re.DOTALL)
    
    return text

def process_image(image_bytes, max_mb=4):
    """å›¾ç‰‡é¢„å¤„ç†ï¼šä¿®æ­£æ—‹è½¬ + æ™ºèƒ½å‹ç¼©"""
    try:
        img = Image.open(io.BytesIO(image_bytes))
        img = ImageOps.exif_transpose(img) # ä¿®æ­£æ‰‹æœºæ‹ç…§æ—‹è½¬
        
        # ä¿®å¤é€æ˜åº•å˜é»‘
        if img.mode != 'RGB':
            bg = Image.new('RGB', img.size, (255, 255, 255))
            if 'A' in img.mode or 'transparency' in img.info:
                img = img.convert('RGBA')
                bg.paste(img, mask=img.split()[-1])
                img = bg
            else:
                img = img.convert('RGB')

        # å‹ç¼©é€»è¾‘
        buf = io.BytesIO()
        img.save(buf, format="JPEG", quality=95)
        
        # åªæœ‰åœ¨å›¾ç‰‡çœŸçš„å¾ˆå¤§ (>4MB) æ—¶æ‰å‹ç¼©
        if len(buf.getvalue()) > max_mb * 1024 * 1024:
            img.save(buf, format="JPEG", quality=85) # ç¨å¾®é™ç‚¹è´¨é‡å³å¯
            
        return buf.getvalue()
    except Exception as e:
        st.error(f"å›¾ç‰‡å¤„ç†å‡ºé”™: {e}")
        return image_bytes

def get_ocr_text(image_bytes):
    """è°ƒç”¨ OCRï¼Œä»£ç ç»“æ„å·²æ‹†è§£ï¼Œé˜²æ­¢æ‹¬å·æŠ¥é”™"""
    client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")

    try:
        # 1. å‡†å¤‡ Base64 å­—ç¬¦ä¸²
        b64_str = base64.b64encode(image_bytes).decode('utf-8')
        
        # 2. å‡†å¤‡æ¶ˆæ¯å†…å®¹ (æ‹†å¼€å†™ï¼Œä¸å¥—å¨ƒ)
        text_part = {"type": "text", "text": "æå–å›¾ä¸­æ‰€æœ‰æ–‡å­—å’ŒLaTeXå…¬å¼ã€‚ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦åŒ…å«JSONæ ¼å¼æˆ–Markdownä»£ç å—ã€‚"}
        image_part = {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{b64_str}",
                "detail": "high" # ğŸ‘ˆ å…³é”®ï¼šå¼ºåˆ¶é«˜æ¸…
            }
        }
        
        # 3. å‘é€è¯·æ±‚
        response = client.chat.completions.create(
            model=OCR_MODEL,
            messages=[{"role": "user", "content": [text_part, image_part]}],
            temperature=0.0,
            top_p=0.7,
            max_tokens=4096
        )
        
        return clean_latex(response.choices[0].message.content)

    except Exception as e:
        st.error(f"OCR è¯·æ±‚å¤±è´¥: {e}")
        return None

def ai_stream(history):
    """è°ƒç”¨å¯¹è¯æ¨¡å‹"""
    client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")
    
    # åªå‘é€æ–‡æœ¬ç»™å¯¹è¯æ¨¡å‹ï¼Œé¿å…å‘å›¾ç‰‡æŠ¥é”™
    clean_history = []
    for msg in history:
        clean_history.append({"role": msg["role"], "content": str(msg["content"])})

    return client.chat.completions.create(
        model=CHAT_MODEL,
        messages=clean_history,
        stream=True,
        temperature=0.7
    )

# ================= ç½‘é¡µä¸»ç¨‹åº =================

st.set_page_config(page_title="AI é¢˜ç›®è®²è§£", layout="centered")
st.title("ğŸ“ AI é¢˜ç›®è®²è§£åŠ©æ‰‹")

if "history" not in st.session_state:
    st.session_state.history = []
if "last_file_id" not in st.session_state:
    st.session_state.last_file_id = None

uploaded_file = st.file_uploader("ğŸ“¸ ä¸Šä¼ é¢˜ç›®å›¾ç‰‡", type=['jpg', 'png', 'jpeg'])

if uploaded_file:
    file_id = f"{uploaded_file.name}-{uploaded_file.size}"
    
    # æ–°å›¾ç‰‡å¤„ç†æµç¨‹
    if st.session_state.last_file_id != file_id:
        st.session_state.last_file_id = file_id
        st.session_state.history = []
        
        with st.status("ğŸš€ æ­£åœ¨è¯†åˆ«é¢˜ç›®...", expanded=True) as status:
            # 1. å¤„ç†å›¾ç‰‡
            img_bytes = process_image(uploaded_file.getvalue())
            # 2. è¯†åˆ«æ–‡å­—
            ocr_result = get_ocr_text(img_bytes)
            
            if ocr_result:
                status.update(label="è¯†åˆ«æˆåŠŸï¼", state="complete", expanded=False)
                
                # åˆå§‹åŒ–å¯¹è¯
                sys_msg = "ä½ æ˜¯ä¸€ä½è€å¸ˆã€‚è¯·è§£æé¢˜ç›®æ€è·¯ï¼Œå…¬å¼ä½¿ç”¨LaTeXæ ¼å¼($...$ æˆ– $$...$$)ã€‚"
                user_msg = f"é¢˜ç›®å†…å®¹å¦‚ä¸‹ï¼š\n{ocr_result}\n\nè¯·è®²è§£ã€‚"
                
                st.session_state.history = [
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": user_msg}
                ]
                
                # è‡ªåŠ¨è§¦å‘è®²è§£
                with st.chat_message("assistant"):
                    ph = st.empty()
                    full_text = ""
                    try:
                        for chunk in ai_stream(st.session_state.history):
                            if chunk.choices[0].delta.content:
                                full_text += chunk.choices[0].delta.content
                                ph.markdown(clean_latex(full_text) + "â–Œ")
                        ph.markdown(clean_latex(full_text))
                        st.session_state.history.append({"role": "assistant", "content": full_text})
                    except Exception as e:
                        st.error(f"è®²è§£å‡ºé”™: {e}")
            else:
                status.update(label="è¯†åˆ«å¤±è´¥", state="error")

# æ˜¾ç¤ºå†å²å¯¹è¯
for msg in st.session_state.history:
    if msg["role"] != "system":
        # è¿™é‡Œçš„åˆ¤æ–­æ˜¯ä¸ºäº†ä¸é‡å¤æ˜¾ç¤ºç¬¬ä¸€æ¡å¾ˆé•¿çš„é¢˜ç›®å†…å®¹ï¼Œä¿æŒç•Œé¢æ¸…çˆ½
        # å¦‚æœä½ æƒ³çœ‹é¢˜ç›®ï¼Œå°±æŠŠä¸‹é¢è¿™ä¸¤è¡Œåˆ æ‰
        if "é¢˜ç›®å†…å®¹å¦‚ä¸‹" in str(msg["content"]) and msg["role"] == "user":
            with st.expander("æŸ¥çœ‹è¯†åˆ«åˆ°çš„é¢˜ç›®"):
                st.markdown(clean_latex(msg["content"]))
            continue
            
        with st.chat_message(msg["role"]):
            st.markdown(clean_latex(msg["content"]))

# è¾“å…¥æ¡†
if query := st.chat_input("å“ªé‡Œä¸æ‡‚ï¼Ÿ"):
    with st.chat_message("user"):
        st.markdown(query)
    st.session_state.history.append({"role": "user", "content": query})
    
    with st.chat_message("assistant"):
        ph = st.empty()
        full_text = ""
        try:
            for chunk in ai_stream(st.session_state.history):
                if chunk.choices[0].delta.content:
                    full_text += chunk.choices[0].delta.content
                    ph.markdown(clean_latex(full_text) + "â–Œ")
            ph.markdown(clean_latex(full_text))
            st.session_state.history.append({"role": "assistant", "content": full_text})
        except Exception as e:
            st.error(f"å‡ºé”™: {e}")
